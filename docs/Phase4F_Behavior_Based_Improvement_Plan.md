# Phase 4F: Behavior-based Approach ê°œì„  ê³„íš

**ì‘ì„±ì¼**: 2025ë…„ 10ì›” 16ì¼
**ê¸°ë°˜**: Phase 4F ì‹¤ì œ ë°ì´í„° ë¶„ì„ ë° Cross-Validation ê²°ê³¼

---

## ìš”ì•½

Phase 4Fì™€ Phase 4D êµì°¨ ê²€ì¦ ê²°ê³¼, **ë‹¨ìˆœ IMU ì„¼ì„œ ê¸°ë°˜ ì ìˆ˜ ì‹œìŠ¤í…œì˜ ì‹¬ê°í•œ í•œê³„**ê°€ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤. README.mdì˜ **Behavior-based approach** ì›ì¹™ì— ë§ì¶°, ì‹¤ì œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.

### í•µì‹¬ ë¬¸ì œì 

| ë¬¸ì œ | í˜„ìƒ | ê·¼ë³¸ ì›ì¸ |
|------|------|-----------|
| **ğŸ”´ ëª¨ë¸ ì„±ëŠ¥ ë¶•ê´´** | Precision 94% â†’ 13.7%, Recall 90% â†’ 0.6% | í•©ì„± ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ì˜ ê´´ë¦¬ |
| **ğŸ”´ ë³€ë³„ë ¥ ë¶€ì¡±** | AGGRESSIVE vs SAFE ì‚¬ê³ ìœ¨ 1.2ë°° (ëª©í‘œ: 4ë°°) | ë‹¨ìˆœ ì´ë²¤íŠ¸ ì¹´ìš´íŠ¸ì˜ í•œê³„ |
| **ğŸ”´ ê³¼ë„í•œ SAFE ë¶„ë¥˜** | 86.1%ê°€ SAFE (Risk Groupì˜ 72.8% í¬í•¨) | ì„ê³„ê°’ ë° ê°€ì¤‘ì¹˜ ë¶ˆì¼ì¹˜ |
| **ğŸ”´ ë‚®ì€ Recall** | ì‹¤ì œ ìœ„í—˜ ìš´ì „ìì˜ 98.8% ë¯¸íƒì§€ | íŠ¹ì§• ê³µê°„ ë¶€ì¡± |

---

## 1. ì‹¤ì œ ë°ì´í„° ë¶„ì„ (Phase 4F)

### 1.1 ë°ì´í„° í’ˆì§ˆ ê²€ì¦ âœ…

**ê¸ì •ì ì¸ ê²°ê³¼:**

```
ì´ ìƒ˜í”Œ: 20,000ê°œ
Risk Group: 10,000 (50.0%)
Safe Group: 10,000 (50.0%)

ì‚¬ê³ ìœ¨:
  Risk: 2,000/10,000 = 20.0%
  Safe: 500/10,000 = 5.0%
  ë¹„ìœ¨: 4.00:1 (ëª©í‘œ ë‹¬ì„±!)
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- âœ… 4:1 ì‚¬ê³ ìœ¨ ë¹„ìœ¨ ì •í™•íˆ ë‹¬ì„± (í˜„ì‹¤ì )
- âœ… ë¼ë²¨ ì •í™•ë„ 85-90% ì¶”ì •
- âœ… ì˜¤ë²„ìƒ˜í”Œë§ ì—†ìŒ (20,000 unique IDs)

### 1.2 ì´ë²¤íŠ¸ íŒ¨í„´ ë¶„ì„

**Risk Group vs Safe Group í‰ê·  ì´ë²¤íŠ¸ ìˆ˜:**

| ì´ë²¤íŠ¸ | Risk Group | Safe Group | **ë¹„ìœ¨** |
|--------|------------|------------|----------|
| ê¸‰ê°€ì† | 2.79 | 0.87 | **3.21x** |
| ê¸‰ì •ê±° | 2.26 | 0.83 | **2.72x** |
| ê¸‰íšŒì „ | 1.90 | 0.74 | **2.55x** |
| ê³¼ì† | 1.38 | 0.51 | **2.69x** |

**í•µì‹¬ ë°œê²¬:**
- âœ… Risk Groupì´ ëª¨ë“  ì´ë²¤íŠ¸ì—ì„œ **2.5~3.2ë°° ë†’ìŒ**
- âœ… ê¸‰ê°€ì†ì´ ê°€ì¥ ë†’ì€ ì°¨ë³„ë ¥ (3.21x)
- âš ï¸ **í•˜ì§€ë§Œ í˜„ì¬ ëª¨ë¸ì€ ì´ë¥¼ ì œëŒ€ë¡œ í™œìš©í•˜ì§€ ëª»í•¨**

### 1.3 í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ (Phase 4F LR ëª¨ë¸)

```
Scenario A & B (ë‘˜ ë‹¤ Precision-focused):
  Threshold: 0.76
  Precision: 50.0%
  Recall: 0.5%  â† ê±°ì˜ ì•„ë¬´ê²ƒë„ ëª» ì°¾ìŒ!
  F1: 1.0%

í˜¼ë™ í–‰ë ¬:
  True Positive: 4
  False Positive: 4
  False Negative: 751  â† ì‹¤ì œ ìœ„í—˜ìì˜ 99.5%ë¥¼ ë†“ì¹¨!
  True Negative: 5,241
```

**ë¬¸ì œì :**
- 6,000ëª… í…ŒìŠ¤íŠ¸ ì¤‘ **ë‹¨ 8ëª…ë§Œ Riskë¡œ ì˜ˆì¸¡**
- ì‹¤ì œ ìœ„í—˜ ìš´ì „ì 755ëª… ì¤‘ **4ëª…ë§Œ ê°ì§€** (0.5%)
- **ì‚¬ì‹¤ìƒ ëª¨ë“  ì‚¬ëŒì„ Safeë¡œ ë¶„ë¥˜**

---

## 2. README.mdì˜ Behavior-based Approach ì›ì¹™

### ì›ë¬¸ ì¸ìš©:

> "ë‹¨ìˆœ ê²°ê³¼(Outcome)ì— ê¸°ë°˜í•œ ì ìˆ˜ë§Œìœ¼ë¡œëŠ” ë³´í—˜ë£Œ ì¡°ì • ì™¸ ì‹¤ì§ˆì ì¸ ê°œì„ ì´ ì–´ë µìŠµë‹ˆë‹¤. DrivingScoreëŠ” ê¸‰ê°€ì†Â·ê¸‰ì œë™Â·ì•¼ê°„ ì£¼í–‰ ë“± **í–‰ë™ ë°ì´í„°ë¥¼ ì§ì ‘ ê³„ëŸ‰í™”**í•´ **ì¦‰ê°ì ì¸ í”¼ë“œë°±**ì„ ì œê³µí•˜ëŠ” Behavior-based ì ‘ê·¼ì„ ì±„íƒí•©ë‹ˆë‹¤."

> "**Calibrate by Truth, Feedback by Behavior** ì›ì¹™ì„ ìš´ì˜ ì „ëµìœ¼ë¡œ ì‚¼ìŠµë‹ˆë‹¤."

### í˜„ì¬ ìƒí™© í‰ê°€:

| ì›ì¹™ | í˜„ì¬ ìƒíƒœ | ë¬¸ì œì  |
|------|-----------|--------|
| **í–‰ë™ ë°ì´í„° ê³„ëŸ‰í™”** | âš ï¸ ë¶€ë¶„ì  | IMUë§Œ ì‚¬ìš©, ë§¥ë½ ì •ë³´ ë¶€ì¡± |
| **ì¦‰ê°ì  í”¼ë“œë°±** | âŒ ë¶ˆê°€ëŠ¥ | ëª¨ë¸ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ (Recall 0.5%) |
| **Calibrate by Truth** | âš ï¸ ë¶€ë¶„ì  | 4:1 ë¹„ìœ¨ì€ ë§ì§€ë§Œ ëª¨ë¸ì´ ë°˜ì˜ ëª» í•¨ |
| **Feedback by Behavior** | âŒ ì‹¤íŒ¨ | 86%ê°€ SAFE â†’ ì°¨ë³„í™” ì—†ìŒ |

---

## 3. ê°œì„  ë°©í–¥: 3ë‹¨ê³„ ì ‘ê·¼ë²•

### Phase A: ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥ (1-2ì£¼)
### Phase B: ì¤‘ê¸° ê°œì„  (1-3ê°œì›”)
### Phase C: ì¥ê¸° ì „ëµ (3-6ê°œì›”)

---

## Phase A: ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥ ê°œì„  (1-2ì£¼)

### A1. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ê°•í™”

**ë¬¸ì œ**: í˜„ì¬ Class Weight (Positive: 4.01, Negative: 0.57)ë¡œë„ ë¶€ì¡±

**ê°œì„ ì•ˆ**:
```python
# í˜„ì¬
class_weight = 'balanced'  # ìë™ ê³„ì‚°

# ì œì•ˆ 1: Manual Override
class_weight = {0: 1, 1: 10}  # ì–‘ì„± í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ëŒ€í­ ì¦ê°€

# ì œì•ˆ 2: Custom Balanced
from sklearn.utils.class_weight import compute_sample_weight
sample_weights = compute_sample_weight('balanced', y_train) * 2
```

**ì˜ˆìƒ íš¨ê³¼**: Recall 0.5% â†’ 30%+ í–¥ìƒ

### A2. ì„ê³„ê°’ ëŒ€í­ í•˜í–¥ ì¡°ì •

**ë¬¸ì œ**: Threshold 0.76ì€ ë„ˆë¬´ ë†’ìŒ (Recall 0.5%)

**ì œì•ˆ**:
```python
# í˜„ì¬ ìµœì í™”
scenario_weights = (0.7, 0.2, 0.1)  # Precision ì¤‘ì‹¬

# ì œì•ˆ: Balanced ì ‘ê·¼
scenario_weights = (0.3, 0.5, 0.2)  # F1 ì¤‘ì‹¬

# ë˜ëŠ” Recall ìš°ì„ 
scenario_weights = (0.2, 0.6, 0.2)  # Recall ì¤‘ì‹¬
```

**ê·¼ê±°**:
- **í–‰ë™ ë³€í™” ìœ ë„**ê°€ ëª©í‘œ â†’ **ìœ„í—˜ìë¥¼ ë” ë§ì´ ì°¾ì•„ì•¼** í•¨
- False PositiveëŠ” ê°ìˆ˜ ê°€ëŠ¥ (í”¼ë“œë°± ì œê³µ ê¸°íšŒ)
- False NegativeëŠ” ì¹˜ëª…ì  (ê°œì„  ê¸°íšŒ ìƒì‹¤)

**ì˜ˆìƒ Threshold**: 0.76 â†’ 0.3~0.4

### A3. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ê¸°ì¡´ ë°ì´í„° í™œìš©)

**í˜„ì¬ íŠ¹ì§• (5ê°œ)**:
- rapid_accel, sudden_stop, sharp_turn, over_speed, is_night

**ì¶”ê°€ íŠ¹ì§• (ë°ì´í„° ê°€ê³µ)**:
```python
# 1. ì´ë²¤íŠ¸ ì´í•©
'total_events': rapid_accel + sudden_stop + sharp_turn + over_speed

# 2. ìœ„í—˜ ì´ë²¤íŠ¸ ë¹„ìœ¨
'risky_event_ratio': (ê¸‰ê°€ì† + ê¸‰ì •ê±°) / max(total_events, 1)

# 3. ì•¼ê°„ ìœ„í—˜ ì´ë²¤íŠ¸
'night_risky_events': (ê¸‰ê°€ì† + ê¸‰ì •ê±°) * is_night * 1.5

# 4. ì´ë²¤íŠ¸ ì¡°í•© (ê¸‰ê°€ì† + ê¸‰ì •ê±°)
'emergency_maneuvers': min(ê¸‰ê°€ì†, ê¸‰ì •ê±°)  # ê¸‰ì •ê±° ì§ì „ ê¸‰ê°€ì†

# 5. ê³¼ì† ì¤‘ ê¸‰íšŒì „
'overspeed_turn': over_speed * sharp_turn

# 6. ì´ë²¤íŠ¸ í‘œì¤€í¸ì°¨ (ë³€ë™ì„±)
# ì—¬ëŸ¬ tripì´ ìˆë‹¤ë©´ í‘œì¤€í¸ì°¨ ê³„ì‚°
```

**ì˜ˆìƒ íš¨ê³¼**: AUC +0.05, F1 +10%p

### A4. SMOTE ëŒ€ì‹  Under-sampling + Class Weight

**ë¬¸ì œ**: í˜„ì¬ SMOTE ë¯¸ì‚¬ìš© (ì˜¤ë²„ìƒ˜í”Œë§ ê¸ˆì§€ ì›ì¹™)

**ì œì•ˆ**: Under-sampling ë³‘í–‰
```python
from imblearn.under_sampling import RandomUnderSampler

# Safe Group ì¼ë¶€ë§Œ ìƒ˜í”Œë§
rus = RandomUnderSampler(sampling_strategy=0.5)  # Risk:Safe = 1:2
X_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)

# Class Weight í•¨ê»˜ ì‚¬ìš©
model = LogisticRegression(class_weight={0: 1, 1: 5})
```

**ì¥ì **:
- ì˜¤ë²„ìƒ˜í”Œë§ ê¸ˆì§€ ì›ì¹™ ì¤€ìˆ˜
- Class ë¶ˆê· í˜• ì™„í™”
- í•™ìŠµ ì†ë„ í–¥ìƒ

---

## Phase B: ì¤‘ê¸° ê°œì„  (1-3ê°œì›”)

### B1. ì•™ìƒë¸” ëª¨ë¸ ê³ ë„í™”

**í˜„ì¬**: LR + ê·œì¹™ ê¸°ë°˜ (ê°„ì†Œí™”)

**ì œì•ˆ**: ì§„ì§œ ì•™ìƒë¸”
```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.ensemble import VotingClassifier

# ëª¨ë¸ êµ¬ì„±
lr = LogisticRegression(class_weight={0:1, 1:10})
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    class_weight='balanced_subsample'
)
gbm = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5
)

# Soft Voting
ensemble = VotingClassifier(
    estimators=[('lr', lr), ('rf', rf), ('gbm', gbm)],
    voting='soft'
)
```

**ì˜ˆìƒ ì„±ëŠ¥**:
- Phase 4D ìˆ˜ì¤€ íšŒë³µ (F1 50%+)
- RandomForestê°€ ë¹„ì„ í˜• íŒ¨í„´ í¬ì°©

### B2. XGBoost + í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

**ì œì•ˆ**:
```python
import xgboost as xgb

# XGBoost with class imbalance
model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.05,
    scale_pos_weight=10,  # Class imbalance ì²˜ë¦¬
    gamma=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='binary:logistic',
    eval_metric='auc'
)

# Grid Search
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'scale_pos_weight': [5, 10, 15]
}

grid_search = GridSearchCV(
    model, param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1
)
```

**ì˜ˆìƒ íš¨ê³¼**: F1 1% â†’ 60%+

### B3. ë§¥ë½ ì •ë³´ ì¶”ê°€ (ë©”íƒ€ë°ì´í„° í™œìš©)

**í˜„ì¬ ë©”íƒ€ë°ì´í„°**:
- sensor_id, city, trip_duration

**í™œìš© ë°©ì•ˆ**:
```python
# 1. ë„ì‹œë³„ ìœ„í—˜ë„ (í†µê³„ ê¸°ë°˜)
city_accident_rate = {
    'New York': 0.15,
    'Los Angeles': 0.12,
    # ... ì‹¤ì œ ì‚¬ê³  ë°ì´í„°ë¡œ ê³„ì‚°
}
features['city_risk'] = city_accident_rate.get(city, 0.1)

# 2. ì£¼í–‰ ì‹œê°„ êµ¬ê°„
features['trip_length_category'] = (
    0 if trip_duration < 30 else
    1 if trip_duration < 60 else
    2  # ì¥ê±°ë¦¬
)

# 3. ì´ë²¤íŠ¸ ë°€ë„
features['event_density'] = total_events / max(trip_duration, 1)

# 4. ë„ì‹œ ê·œëª¨
big_cities = ['New York', 'Los Angeles', 'Chicago']
features['is_big_city'] = 1 if city in big_cities else 0
```

### B4. ìƒëŒ€ í‰ê°€ ì‹œìŠ¤í…œ (Percentile-based)

**ë¬¸ì œ**: ì ˆëŒ€ ì ìˆ˜ë¡œëŠ” ë³€ë³„ë ¥ ë¶€ì¡±

**ì œì•ˆ**:
```python
# 1. ì ìˆ˜ ê³„ì‚°
scores = model.predict_proba(X_test)[:, 1]

# 2. ë°±ë¶„ìœ„ìˆ˜ ë³€í™˜
from scipy.stats import percentileofscore

percentiles = [percentileofscore(scores, s) for s in scores]

# 3. ë“±ê¸‰ ë¶€ì—¬
def assign_grade(percentile):
    if percentile >= 90:
        return 'SAFE'
    elif percentile >= 75:
        return 'MODERATE'
    else:
        return 'AGGRESSIVE'

# 4. ìë™ìœ¼ë¡œ ë¶„í¬ ì¡°ì •
# SAFE 65%, MODERATE 25%, AGGRESSIVE 10% ë‹¬ì„±
```

**ì¥ì **:
- ë¶„í¬ ìë™ ì¡°ì •
- ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ê¸°ì¤€ ë³€í™” ëŒ€ì‘
- ì‚¬ìš©ì ê°„ ìƒëŒ€ ë¹„êµ ê°€ëŠ¥

---

## Phase C: ì¥ê¸° ì „ëµ (3-6ê°œì›”)

### C1. ì™¸ë¶€ ë°ì´í„° í†µí•©

**1) ë‚ ì”¨ ë°ì´í„° (API)**
```python
# OpenWeatherMap API
weather_features = {
    'rain': 1 if 'rain' in weather else 0,
    'snow': 1 if 'snow' in weather else 0,
    'temperature': temp,
    'visibility': visibility_km
}
```

**2) êµí†µ ì •ë³´**
```python
# Google Maps Traffic API
traffic_features = {
    'traffic_level': 0-3,  # 0=ì›í™œ, 3=ì •ì²´
    'rush_hour': 1 if 7-9ì‹œ or 17-19ì‹œ else 0
}
```

**3) ë„ë¡œ ìœ í˜• (OSM)**
```python
# OpenStreetMap
road_features = {
    'highway': 1,
    'urban': 1,
    'residential': 0
}
```

### C2. ì‹œê³„ì—´ ëª¨ë¸ (LSTM/Transformer)

**í˜„ì¬**: ê° trip ë…ë¦½ì  í‰ê°€

**ì œì•ˆ**: ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
```python
import torch
import torch.nn as nn

class DrivingScoreLSTM(nn.Module):
    def __init__(self, input_size=5, hidden_size=64):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, 2)

    def forward(self, x):
        # x: (batch, sequence_length, features)
        _, (hidden, _) = self.lstm(x)
        return self.fc(hidden[-1])

# ì—¬ëŸ¬ tripì„ sequenceë¡œ í•™ìŠµ
# ìš´ì „ íŒ¨í„´ì˜ ì‹œê°„ì  ë³€í™” í¬ì°©
```

**í™œìš©**:
- ìµœê·¼ Nê°œ tripì˜ íŒ¨í„´ ë¶„ì„
- ê°œì„ /ì•…í™” ì¶”ì„¸ ê°ì§€
- ì¥ê¸° í–‰ë™ íŒ¨í„´ í•™ìŠµ

### C3. ì„¤ëª… ê°€ëŠ¥í•œ AI (Explainable AI)

**ëª©ì **: ì‚¬ìš©ìì—ê²Œ "ì™œ ì´ ì ìˆ˜ì¸ê°€?" ì„¤ëª…

**ë°©ë²• 1: SHAP**
```python
import shap

explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# ì‚¬ìš©ìë³„ ì„¤ëª…
for i, sample in enumerate(X_test[:5]):
    print(f"\nì‚¬ìš©ì {i}ì˜ ì ìˆ˜ ìš”ì¸:")
    for feature, shap_val in zip(feature_names, shap_values[i]):
        print(f"  {feature}: {shap_val:+.2f}")
```

**ë°©ë²• 2: Feature Importance ì‹œê°í™”**
```python
# XGBoost Feature Importance
importance = xgb_model.feature_importances_

# ì‚¬ìš©ì í”¼ë“œë°±
feedback = {
    'score': 45,
    'grade': 'AGGRESSIVE',
    'top_issues': [
        'ê¸‰ê°€ì† íšŸìˆ˜ê°€ í‰ê· ë³´ë‹¤ 3ë°° ë†’ìŠµë‹ˆë‹¤',
        'ì•¼ê°„ ì£¼í–‰ ì¤‘ ê¸‰ì •ê±° ë¹ˆë²ˆ',
        'ê³¼ì† êµ¬ê°„ì—ì„œ ê¸‰íšŒì „ ë°œìƒ'
    ],
    'recommendations': [
        'ë¶€ë“œëŸ¬ìš´ ê°€ì† ì—°ìŠµ',
        'ì•¼ê°„ ì£¼í–‰ ì‹œ ì†ë„ ì¤„ì´ê¸°',
        'ì»¤ë¸Œ ì „ ê°ì†'
    ]
}
```

### C4. ê°•í™”í•™ìŠµ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜

**ì•„ì´ë””ì–´**: ì‚¬ìš©ì í–‰ë™ ë³€í™”ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¡°ì •

```python
# Q-Learning ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™”
class AdaptiveWeightOptimizer:
    def __init__(self):
        self.weights = {
            'rapid_accel': 3.0,
            'sudden_stop': 3.0,
            'sharp_turn': 2.0,
            'over_speed': 2.0
        }

    def update(self, user_feedback, accident_occurred):
        # ì‚¬ê³  ë°œìƒ ì‹œ í•´ë‹¹ ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ ì¦ê°€
        if accident_occurred:
            for event, count in user_feedback.items():
                if count > threshold:
                    self.weights[event] *= 1.1

        # ê°œì„  ì‹œ ê°€ì¤‘ì¹˜ ì•ˆì •í™”
        else:
            for event in self.weights:
                self.weights[event] *= 0.99

# ê°œì¸í™”ëœ ì ìˆ˜ ì‹œìŠ¤í…œ
```

---

## 4. ì‹¤í–‰ ìš°ì„ ìˆœìœ„ ë° ë¡œë“œë§µ

### Week 1-2: Quick Wins (Phase A)

**ëª©í‘œ**: Recall 0.5% â†’ 30%+ ë‹¬ì„±

| ë²ˆí˜¸ | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ | ì˜ˆìƒ íš¨ê³¼ |
|------|------|-----------|-----------|
| 1 | Class Weight ì¦ê°€ (1â†’10) | 1ì‹œê°„ | Recall +20%p |
| 2 | Threshold í•˜í–¥ (0.76â†’0.35) | 2ì‹œê°„ | Recall +10%p |
| 3 | íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (6ê°œ ì¶”ê°€) | 1ì¼ | F1 +10%p |
| 4 | Scenario Bë¥¼ Recall-focusedë¡œ ë³µì› | 2ì‹œê°„ | ì„ íƒì§€ ì œê³µ |

**ê²€ì¦**:
```bash
cd research
python phase4f_step3_model_training_improved.py
```

### Week 3-4: ì•™ìƒë¸” ì ìš© (Phase B1-B2)

**ëª©í‘œ**: F1 1% â†’ 50%+ ë‹¬ì„±

| ë²ˆí˜¸ | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ | ì˜ˆìƒ íš¨ê³¼ |
|------|------|-----------|-----------|
| 1 | RandomForest ì¶”ê°€ | 1ì¼ | +15%p F1 |
| 2 | XGBoost ì ìš© | 2ì¼ | +20%p F1 |
| 3 | Voting Ensemble | 1ì¼ | +5%p F1 |
| 4 | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | 2ì¼ | +10%p F1 |

### Month 2: ë§¥ë½ ì •ë³´ (Phase B3-B4)

**ëª©í‘œ**: ì‹¤ì œ ë°°í¬ ì¤€ë¹„

| ë²ˆí˜¸ | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ | ì˜ˆìƒ íš¨ê³¼ |
|------|------|-----------|-----------|
| 1 | ë©”íƒ€ë°ì´í„° íŠ¹ì§• ì¶”ê°€ | 3ì¼ | +5%p AUC |
| 2 | ìƒëŒ€ í‰ê°€ ì‹œìŠ¤í…œ | 5ì¼ | ë¶„í¬ ê°œì„  |
| 3 | A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ | 3ì¼ | ê²€ì¦ ì²´ê³„ |
| 4 | ìµœì¢… ê²€ì¦ | 4ì¼ | - |

### Month 3-6: ì¥ê¸° ì „ëµ (Phase C)

**ì„ íƒì  ì§„í–‰**

---

## 5. ì„±ê³µ ì§€í‘œ (Success Metrics)

### 5.1 ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ

**ìµœì†Œ ëª©í‘œ (Phase A ì™„ë£Œ ì‹œ)**:
- Precision: â‰¥ 40%
- Recall: â‰¥ 30%
- F1: â‰¥ 30%

**ëª©í‘œ (Phase B ì™„ë£Œ ì‹œ)**:
- Precision: â‰¥ 70%
- Recall: â‰¥ 60%
- F1: â‰¥ 65%

**ìµœì¢… ëª©í‘œ (Phase C)**:
- Precision: â‰¥ 80%
- Recall: â‰¥ 70%
- F1: â‰¥ 75%
- AUC: â‰¥ 0.85

### 5.2 ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ

**ë³€ë³„ë ¥**:
- AGGRESSIVE / SAFE ì‚¬ê³ ìœ¨ ë¹„ìœ¨: â‰¥ 3:1 (í˜„ì¬ 1.2:1)

**ë¶„í¬**:
- SAFE: 60-70% (í˜„ì¬ 86.1%)
- MODERATE: 20-30% (í˜„ì¬ 13.4%)
- AGGRESSIVE: 5-15% (í˜„ì¬ 0.6%)

**í–‰ë™ ë³€í™”**:
- ìœ„í—˜ ìš´ì „ì íƒì§€ìœ¨: â‰¥ 70% (í˜„ì¬ 1.2%)
- ì•ˆì „ ìš´ì „ì ì •í™•ë„: â‰¥ 90% (í˜„ì¬ 99.4% - ìœ ì§€)

---

## 6. ìœ„í—˜ ìš”ì¸ ë° ëŒ€ì‘ ë°©ì•ˆ

### 6.1 ë°ì´í„° í’ˆì§ˆ

**ìœ„í—˜**: ë¼ë²¨ ë…¸ì´ì¦ˆ 10-15%

**ëŒ€ì‘**:
- ì•™ìƒë¸” ëª¨ë¸ë¡œ ë…¸ì´ì¦ˆ robust
- Cross-validationìœ¼ë¡œ ê²€ì¦
- ì´ìƒì¹˜ íƒì§€ ë° ì œê±°

### 6.2 ê³¼ì í•©

**ìœ„í—˜**: í•™ìŠµ ë°ì´í„° 20Kë¡œ ì œí•œì 

**ëŒ€ì‘**:
- Regularization (L1/L2)
- Early stopping
- K-fold CV

### 6.3 íŠ¹ì§• ë¶€ì¡±

**ìœ„í—˜**: IMU ì„¼ì„œë§Œìœ¼ë¡œ í•œê³„

**ëŒ€ì‘**:
- íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ìš°ì„ 
- ì™¸ë¶€ ë°ì´í„°ëŠ” ì„ íƒì 
- ë©”íƒ€ë°ì´í„° ìµœëŒ€ í™œìš©

---

## 7. í•µì‹¬ ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì‹¤í–‰ (ì´ë²ˆ ì£¼)

1. **Class Weightë¥¼ 10ìœ¼ë¡œ ì¦ê°€**
   ```python
   class_weight = {0: 1, 1: 10}
   ```

2. **Scenario Bë¥¼ Recall-focusedë¡œ ë³µì›**
   ```python
   scenario_b_weights = (0.2, 0.7, 0.1)  # Precision, Recall, F1
   ```

3. **Threshold ë²”ìœ„ í™•ì¥**
   ```python
   thresholds = [i / 100 for i in range(5, 91)]  # 0.05 ~ 0.90
   ```

4. **íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ 3ê°œ ì¶”ê°€**
   - total_events
   - risky_event_ratio
   - night_risky_events

### ë‹¤ìŒ ë‹¬

5. **XGBoost ì ìš©**
   - scale_pos_weight=10
   - max_depth=6
   - GridSearchCV

6. **ë©”íƒ€ë°ì´í„° í™œìš©**
   - city_risk
   - trip_duration_category
   - event_density

7. **ìƒëŒ€ í‰ê°€ ì‹œìŠ¤í…œ**
   - Percentile ê¸°ë°˜
   - ìë™ ë¶„í¬ ì¡°ì •

---

## 8. ê²°ë¡ 

### í˜„ì¬ ìƒí™© ìš”ì•½

**âœ… ì˜ ëœ ì **:
- 4:1 ì‚¬ê³ ìœ¨ ë¹„ìœ¨ ì •í™•íˆ ë‹¬ì„±
- ê³ í’ˆì§ˆ ë°ì´í„° 20K í™•ë³´
- Risk Group ì´ë²¤íŠ¸ 2.5~3.2ë°° ì°¨ì´ í™•ì¸

**ğŸ”´ ë¬¸ì œì **:
- ëª¨ë¸ ì„±ëŠ¥ ë¶•ê´´ (Recall 0.5%)
- ë³€ë³„ë ¥ ë¶€ì¡± (1.2ë°°)
- ì‹¤ìš©ì„± ì—†ìŒ (86%ê°€ SAFE)

### Behavior-based Approach ì‹¤í˜„ì„ ìœ„í•œ í•µì‹¬

**ì›ì¹™ ì¬í™•ì¸**:
> "ê¸‰ê°€ì†Â·ê¸‰ì œë™Â·ì•¼ê°„ ì£¼í–‰ ë“± **í–‰ë™ ë°ì´í„°ë¥¼ ì§ì ‘ ê³„ëŸ‰í™”**í•´ **ì¦‰ê°ì ì¸ í”¼ë“œë°±**ì„ ì œê³µ"

**ì‹¤í˜„ ë°©ì•ˆ**:

1. **Recall ìš°ì„  ìµœì í™”** â†’ ìœ„í—˜ ìš´ì „ìë¥¼ ë¨¼ì € ì°¾ì•„ì•¼ í–‰ë™ ë³€í™” ìœ ë„ ê°€ëŠ¥
2. **íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§** â†’ í–‰ë™ì˜ ë§¥ë½(context) ë°˜ì˜
3. **ì•™ìƒë¸” ëª¨ë¸** â†’ ë¹„ì„ í˜• íŒ¨í„´ í¬ì°©
4. **ìƒëŒ€ í‰ê°€** â†’ ì§€ì†ì ì¸ ê°œì„  ë™ê¸° ë¶€ì—¬
5. **ì„¤ëª… ê°€ëŠ¥ì„±** â†’ "ì™œ ì´ ì ìˆ˜?" ëª…í™•íˆ ì „ë‹¬

### ë‹¤ìŒ ë‹¨ê³„

**Week 1-2 (ì¦‰ì‹œ)**:
```bash
cd research
# 1. ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •
# - Class Weight: 10
# - Scenario B: Recall-focused
# - Feature Engineering: 3ê°œ ì¶”ê°€

python phase4f_step3_model_training_v2.py
python phase4f_step4_final_report.py
```

**Week 3-4 (ì•™ìƒë¸”)**:
```bash
python phase4f_step3_ensemble.py  # XGBoost + RF + LR
```

**Month 2 (ê²€ì¦)**:
```bash
python phase4f_ab_test.py  # ì‹¤ì œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸
```

---

## ë¶€ë¡

### A. ì°¸ê³  ë¬¸ì„œ

- README.md: Behavior-based approach ì›ì¹™
- Phase4F_Final_Report.md: í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥
- Phase4D_4F_Cross_Validation_Report.md: ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„
- Phase4D_Model_Improvement.md: Phase 4D ì„±ê³µ ì‚¬ë¡€

### B. ì½”ë“œ í…œí”Œë¦¿

**ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„  ì½”ë“œ**:

```python
# phase4f_step3_model_training_v2.py

# 1. Class Weight ì¦ê°€
weight_positive = n_samples / (2 * n_positive) * 2.5  # 2.5ë°° ì¦ê°€

# 2. Scenario Bë¥¼ Recall-focusedë¡œ
scenario_b_weights = (0.2, 0.7, 0.1)

# 3. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
def add_engineered_features(sample):
    f = sample['features']

    # Total events
    f['total_events'] = (f['rapid_accel'] + f['sudden_stop'] +
                        f['sharp_turn'] + f['over_speed'])

    # Risky event ratio
    f['risky_ratio'] = (f['rapid_accel'] + f['sudden_stop']) / max(f['total_events'], 1)

    # Night risky events
    f['night_risky'] = (f['rapid_accel'] + f['sudden_stop']) * f['is_night'] * 1.5

    return sample

# 4. Threshold ë²”ìœ„ í™•ì¥
thresholds = [i / 100 for i in range(5, 91)]  # 0.05 ~ 0.90
```

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025ë…„ 10ì›” 16ì¼
**ë‹¤ìŒ ë¦¬ë·°**: Week 1-2 ê°œì„  í›„
